{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Problem_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahera02/machine_learning/blob/master/ML_Problem_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "IhgDA9OXORVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading the MNIST data set through Keras**"
      ]
    },
    {
      "metadata": {
        "id": "JKNPsLcH8Pq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fdfebac5-3383-405a-ebb1-13ab1fc0eafe"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib as plt\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "E17aZ27cOJmC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preparing the image data and Preparing the train labels ( converting intergers to binary)**"
      ]
    },
    {
      "metadata": {
        "id": "eX-Nbh48OFyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ed84d466-a0a5-47df-e82b-6a2613633226"
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "train_images = train_images.T\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "test_images = test_images.T\n",
        "\n",
        "train_labels = to_categorical(train_labels_original,10)\n",
        "test_labels = to_categorical(test_labels_original,10)\n",
        "\n",
        "train_labels = train_labels.T\n",
        "test_labels = test_labels.T\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels_original.shape)\n",
        "print(test_labels_original.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 60000)\n",
            "(784, 10000)\n",
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lo7laqUhRzY8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initialize the parameters epochs,batch size and learning rate for training the data set**"
      ]
    },
    {
      "metadata": {
        "id": "IMbit1W-Rvdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn_rate = 0.05\n",
        "batch_size = 32\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ouT95pP2S8C1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method Definitions for logistic regression, softmax and categorical cross entropy **\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HF8RxYjmS4O3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize weights and bias\n",
        "def init():\n",
        "  W = np.random.randn(10,784)*0.01\n",
        "  b = np.zeros(shape=(10, 1))\n",
        "  return W,b\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
        "  \n",
        "# forward propagation\n",
        "def forward_propagation(weight,bias,x):\n",
        "  logist_reg = np.dot(weight,np.transpose(x)) + (bias)\n",
        "  result = softmax(logist_reg)\n",
        "  return result  \n",
        "  \n",
        "# Function to calculate categorical cross entropy loss\n",
        "def categorical_cross_entropy(Y, Y_new):\n",
        "    return -np.mean(Y * np.log(Y_new))\n",
        "  \n",
        "\n",
        "def get_mini_batches(train_images,train_labels,batch_size):\n",
        "  for i in range(0, train_images.shape[0]- batch_size+1, batch_size):\n",
        "    last_slice = slice(i, i + batch_size)\n",
        "    yield train_images[last_slice],train_labels[last_slice]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyPu5qKCMDDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training the network using mini-batch stochastic gradient**"
      ]
    },
    {
      "metadata": {
        "id": "-qLoZEeta8Fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9d84b4b4-4320-4a7c-d0f1-1dafa921f3b1"
      },
      "cell_type": "code",
      "source": [
        "#  Model with no hidden layer\n",
        "  weight,bias = init()\n",
        "  \n",
        "  #training\n",
        "  for i in range(epochs):\n",
        "     \n",
        "     #using mini batch stochastic gradient descent\n",
        "     for batch in get_mini_batches(train_images.T, train_labels.T, batch_size):\n",
        " \n",
        "        train_images_batch, train_labels_batch = batch\n",
        "        train_images_batch = train_images_batch.T\n",
        "        train_labels_batch = train_labels_batch.T\n",
        "  \n",
        "        m = train_images_batch.shape[0]\n",
        "        \n",
        "        #forward propogation\n",
        "        result = forward_propagation(weight,bias,train_images_batch.T)\n",
        "\n",
        "        loss = categorical_cross_entropy(train_labels_batch, result)\n",
        "        \n",
        "        #back propogation\n",
        "        dZ = result-train_labels_batch\n",
        "\n",
        "        dweight = (1/m) * np.dot(dZ, train_images_batch.T)\n",
        "        dbias = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "        weight = weight - learn_rate * dweight\n",
        "        bias = bias - learn_rate * dbias\n",
        "    \n",
        "  result1 = forward_propagation(weight,bias,train_images.T)\n",
        "  \n",
        "  result2 = forward_propagation(weight,bias,test_images.T)\n",
        "  \n",
        "  # creating one hot encoding of the softmax ouptut\n",
        "  train_labels_prediction = np.zeros_like(train_labels.T)\n",
        "  train_labels_prediction[np.arange(len(result1.T)), result1.T.argmax(1)] = 1\n",
        "  \n",
        "  test_labels_prediction = np.zeros_like(test_labels.T)\n",
        "  test_labels_prediction[np.arange(len(result2.T)), result2.T.argmax(1)] = 1\n",
        "  \n",
        "  \n",
        "  # Training and Testing Accuracy\n",
        "  print(\"Training accuracy : {} %\".format(100 - np.mean(np.abs(train_labels_prediction - train_labels.T)) * 100))\n",
        "  print(\"Testing accuracy  : {} %\".format(100 - np.mean(np.abs(test_labels_prediction - test_labels.T)) * 100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy : 98.02100006490946 %\n",
            "test accuracy  : 98.14800005406141 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}