{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Problem_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahera02/machine_learning/blob/master/ML_Problem_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "IhgDA9OXORVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading the MNIST data set through Keras**"
      ]
    },
    {
      "metadata": {
        "id": "JKNPsLcH8Pq6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib as plt\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E17aZ27cOJmC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preparing the image data and Preparing the train labels ( converting intergers to binary)**"
      ]
    },
    {
      "metadata": {
        "id": "eX-Nbh48OFyv",
        "colab_type": "code",
        "outputId": "ba6966bb-4f00-4815-c0e5-2db1b1ff06d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "train_images = train_images.T\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "test_images = test_images.T\n",
        "\n",
        "train_labels = to_categorical(train_labels_original,10)\n",
        "test_labels = to_categorical(test_labels_original,10)\n",
        "\n",
        "train_labels = train_labels.T\n",
        "test_labels = test_labels.T\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels_original.shape)\n",
        "print(test_labels_original.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 60000)\n",
            "(784, 10000)\n",
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lo7laqUhRzY8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initialize the parameters epochs,batch size and learning rate for training the data set**"
      ]
    },
    {
      "metadata": {
        "id": "IMbit1W-Rvdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn_rate = 0.05\n",
        "batch_size = 32\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ouT95pP2S8C1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method Definitions for logistic regression, softmax and categorical cross entropy **\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HF8RxYjmS4O3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize weights and bias\n",
        "def init():\n",
        "  W = np.random.randn(10,784)*0.01\n",
        "  b = np.zeros(shape=(10, 1))\n",
        "  return W,b\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
        "  \n",
        "# forward propagation\n",
        "def forward_propagation(weight,bias,x):\n",
        "  logist_reg = np.dot(weight,np.transpose(x)) + (bias)\n",
        "  result = softmax(logist_reg)\n",
        "  return result  \n",
        "  \n",
        "# Function to calculate categorical cross entropy loss\n",
        "def categorical_cross_entropy(Y, Y_new):\n",
        "    return -np.mean(Y * np.log(Y_new))\n",
        "  \n",
        "\n",
        "def get_mini_batches(train_images,train_labels,batch_size):\n",
        "  for i in range(0, train_images.shape[0]- batch_size+1, batch_size):\n",
        "    last_slice = slice(i, i + batch_size)\n",
        "    yield train_images[last_slice],train_labels[last_slice]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyPu5qKCMDDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training the network using mini-batch stochastic gradient**"
      ]
    },
    {
      "metadata": {
        "id": "-qLoZEeta8Fq",
        "colab_type": "code",
        "outputId": "aa80ce66-1498-4981-bcf3-999c94a949a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "cell_type": "code",
      "source": [
        "#  Model with no hidden layer\n",
        "  weight,bias = init()\n",
        "  \n",
        "  #training\n",
        "  for i in range(epochs):\n",
        "     \n",
        "     #using mini batch stochastic gradient descent\n",
        "     for batch in get_mini_batches(train_images.T, train_labels.T, batch_size):\n",
        " \n",
        "        train_images_batch, train_labels_batch = batch\n",
        "        train_images_batch = train_images_batch.T\n",
        "        train_labels_batch = train_labels_batch.T\n",
        "  \n",
        "        m = train_images_batch.shape[0]\n",
        "        \n",
        "        #forward propogation\n",
        "        result = forward_propagation(weight,bias,train_images_batch.T)\n",
        "\n",
        "        loss = categorical_cross_entropy(train_labels_batch, result)\n",
        "        \n",
        "        #back propogation\n",
        "        dZ = result-train_labels_batch\n",
        "\n",
        "        dweight = (1/m) * np.dot(dZ, train_images_batch.T)\n",
        "        dbias = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "        weight = weight - learn_rate * dweight\n",
        "        bias = bias - learn_rate * dbias\n",
        "     print(\"Loss at Epoch \", i, \" \", loss)\n",
        "    \n",
        "  result1 = forward_propagation(weight,bias,train_images.T)\n",
        "  \n",
        "  result2 = forward_propagation(weight,bias,test_images.T)\n",
        "  \n",
        "  # creating one hot encoding of the softmax ouptut\n",
        "  train_labels_prediction = np.zeros_like(train_labels.T)\n",
        "  train_labels_prediction[np.arange(len(result1.T)), result1.T.argmax(1)] = 1\n",
        "  \n",
        "  test_labels_prediction = np.zeros_like(test_labels.T)\n",
        "  test_labels_prediction[np.arange(len(result2.T)), result2.T.argmax(1)] = 1\n",
        "  \n",
        "  \n",
        "  # Training and Testing Accuracy\n",
        "  print(\"Training accuracy : {} %\".format(100 - np.mean(np.abs(train_labels_prediction - train_labels.T)) * 100))\n",
        "  print(\"Testing accuracy  : {} %\".format(100 - np.mean(np.abs(test_labels_prediction - test_labels.T)) * 100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at Epoch  0   0.08678900701856772\n",
            "Loss at Epoch  1   0.058708707066656304\n",
            "Loss at Epoch  2   0.046442500190412586\n",
            "Loss at Epoch  3   0.03937971471463951\n",
            "Loss at Epoch  4   0.034745240513284904\n",
            "Loss at Epoch  5   0.03145774978675635\n",
            "Loss at Epoch  6   0.028999937675183513\n",
            "Loss at Epoch  7   0.027090882961609664\n",
            "Loss at Epoch  8   0.02556411905224698\n",
            "Loss at Epoch  9   0.024314455500147224\n",
            "Loss at Epoch  10   0.023272051932326752\n",
            "Loss at Epoch  11   0.022388679118929992\n",
            "Loss at Epoch  12   0.021629947759072114\n",
            "Loss at Epoch  13   0.020970675528919864\n",
            "Loss at Epoch  14   0.020392002675689015\n",
            "Loss at Epoch  15   0.019879529682623374\n",
            "Loss at Epoch  16   0.01942207701286918\n",
            "Loss at Epoch  17   0.019010836802237984\n",
            "Loss at Epoch  18   0.01863877900447326\n",
            "Loss at Epoch  19   0.018300227100461662\n",
            "Training accuracy : 98.02166670560837 %\n",
            "Testing accuracy  : 98.13400004059076 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}